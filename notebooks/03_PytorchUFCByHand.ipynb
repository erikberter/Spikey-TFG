{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8016956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import UCF101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e194ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf_data_dir = \"datasets\\\\UFC101\\\\UCF101\\\\UCF-101\"\n",
    "ucf_label_dir = \"datasets\\\\UFC101\\\\ucfTrainTestlist\"\n",
    "frames_per_clip = 4\n",
    "step_between_clips = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a078d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/jfzhang95/pytorch-video-recognition/blob/master/mypath.py\n",
    "class Path(object):\n",
    "    @staticmethod\n",
    "    def db_dir(database):\n",
    "        if database == 'ucf101':\n",
    "            # folder that contains class labels\n",
    "            root_dir = 'datasets\\\\UFC101\\\\UCF101\\\\UCF-101'\n",
    "\n",
    "            # Save preprocess data into output_dir\n",
    "            output_dir = 'datasets\\\\UFC101\\\\UCF101\\\\Processed'\n",
    "\n",
    "            return root_dir, output_dir\n",
    "        elif database == 'hmdb51':\n",
    "            # folder that contains class labels\n",
    "            root_dir = '/Path/to/hmdb-51'\n",
    "\n",
    "            output_dir = '/path/to/VAR/hmdb51'\n",
    "\n",
    "            return root_dir, output_dir\n",
    "        else:\n",
    "            print('Database {} not available.'.format(database))\n",
    "            raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def model_dir():\n",
    "        return '/path/to/Models/c3d-pretrained.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c620acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder datasets\\UFC101\\UCF101\\Processed\\test ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'Hammering', 'HammerThrow', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpingJack', 'JumpRope', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n",
      "Label ApplyEyeMakeup\n",
      "Label ApplyLipstick\n",
      "Label Archery\n",
      "Label BabyCrawling\n",
      "Label BalanceBeam\n",
      "Label BandMarching\n",
      "Label BaseballPitch\n",
      "Label Basketball\n",
      "Label BasketballDunk\n",
      "Label BenchPress\n",
      "Label Biking\n",
      "Label Billiards\n",
      "Label BlowDryHair\n",
      "Label BlowingCandles\n",
      "Label BodyWeightSquats\n",
      "Label Bowling\n",
      "Label BoxingPunchingBag\n",
      "Label BoxingSpeedBag\n",
      "Label BreastStroke\n",
      "Label BrushingTeeth\n",
      "Label CleanAndJerk\n",
      "Label CliffDiving\n",
      "Label CricketBowling\n",
      "Label CricketShot\n",
      "Label CuttingInKitchen\n",
      "Label Diving\n",
      "Label Drumming\n",
      "Label Fencing\n",
      "Label FieldHockeyPenalty\n",
      "Label FloorGymnastics\n",
      "Label FrisbeeCatch\n",
      "Label FrontCrawl\n",
      "Label GolfSwing\n",
      "Label Haircut\n",
      "Label HammerThrow\n",
      "Label Hammering\n",
      "Label HandstandPushups\n",
      "Label HandstandWalking\n",
      "Label HeadMassage\n",
      "Label HighJump\n",
      "Label HorseRace\n",
      "Label HorseRiding\n",
      "Label HulaHoop\n",
      "Label IceDancing\n",
      "Label JavelinThrow\n",
      "Label JugglingBalls\n",
      "Label JumpRope\n",
      "Label JumpingJack\n",
      "Label Kayaking\n",
      "Label Knitting\n",
      "Label LongJump\n",
      "Label Lunges\n",
      "Label MilitaryParade\n",
      "Label Mixing\n",
      "Label MoppingFloor\n",
      "Label Nunchucks\n",
      "Label ParallelBars\n",
      "Label PizzaTossing\n",
      "Label PlayingCello\n",
      "Label PlayingDaf\n",
      "Label PlayingDhol\n",
      "Label PlayingFlute\n",
      "Label PlayingGuitar\n",
      "Label PlayingPiano\n",
      "Label PlayingSitar\n",
      "Label PlayingTabla\n",
      "Label PlayingViolin\n",
      "Label PoleVault\n",
      "Label PommelHorse\n",
      "Label PullUps\n",
      "Label Punch\n",
      "Label PushUps\n",
      "Label Rafting\n",
      "Label RockClimbingIndoor\n",
      "Label RopeClimbing\n",
      "Label Rowing\n",
      "Label SalsaSpin\n",
      "Label ShavingBeard\n",
      "Label Shotput\n",
      "Label SkateBoarding\n",
      "Label Skiing\n",
      "Label Skijet\n",
      "Label SkyDiving\n",
      "Label SoccerJuggling\n",
      "Label SoccerPenalty\n",
      "Label StillRings\n",
      "Label SumoWrestling\n",
      "Label Surfing\n",
      "Label Swing\n",
      "Label TableTennisShot\n",
      "Label TaiChi\n",
      "Label TennisSwing\n",
      "Label ThrowDiscus\n",
      "Label TrampolineJumping\n",
      "Label Typing\n",
      "Label UnevenBars\n",
      "Label VolleyballSpiking\n",
      "Label WalkingWithDog\n",
      "Label WallPushups\n",
      "Label WritingOnBoard\n",
      "Label YoYo\n",
      "Number of test videos: 2701\n",
      "2701\n",
      "torch.Size([32, 3, 8, 112, 112])\n",
      "tensor([ 46,  66,  12,  69,  13,  13,  89,  91,  70,  34,  26,  93,  36,  19,\n",
      "         41, 100,  89,  51,   6,   5,  41,   9,  39,  36,   2,  79,  56,  31,\n",
      "         34,  75,  15,  59], dtype=torch.int32)\n",
      "torch.Size([32, 3, 8, 112, 112])\n",
      "tensor([ 34,  35,  60,  37,  83,  68,  34,  28, 100,  90,  41,  49,  45,  67,\n",
      "          4,   8,  71,  79,  72,  70,  74,  25,  51,  10,  87,  52,  32,  57,\n",
      "         75,  45,  42,  64], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    r\"\"\"A Dataset for a folder of videos. Expects the directory structure to be\n",
    "    directory->[train/val/test]->[class labels]->[videos]. Initializes with a list\n",
    "    of all file names, along with an array of labels, with label being automatically\n",
    "    inferred from the respective folder names.\n",
    "        Args:\n",
    "            dataset (str): Name of dataset. Defaults to 'ucf101'.\n",
    "            split (str): Determines which folder of the directory the dataset will read from. Defaults to 'train'.\n",
    "            clip_len (int): Determines how many frames are there in each clip. Defaults to 16.\n",
    "            preprocess (bool): Determines whether to preprocess dataset. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset='ucf101', split='train', clip_len=16, preprocess=False):\n",
    "        self.root_dir, self.output_dir = Path.db_dir(dataset)\n",
    "        folder = os.path.join(self.output_dir, split)\n",
    "        self.clip_len = clip_len\n",
    "        self.split = split\n",
    "\n",
    "        # The following three parameters are chosen as described in the paper section 4.1\n",
    "        self.resize_height = 128\n",
    "        self.resize_width = 171\n",
    "        self.crop_size = 112\n",
    "\n",
    "        if not self.check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You need to download it from official website.')\n",
    "\n",
    "        if (not self.check_preprocess()) or preprocess:\n",
    "            print('Preprocessing of {} dataset, this will take long, but it will be done only once.'.format(dataset))\n",
    "            self.preprocess()\n",
    "\n",
    "        # Obtain all the filenames of files inside all the class folders\n",
    "        # Going through each class folder one at a time\n",
    "        self.fnames, labels = [], []\n",
    "        print(f\"Folder {folder} {os.listdir(folder)}\")\n",
    "        for label in sorted(os.listdir(folder)):\n",
    "            print(f\"Label {label}\")\n",
    "            for fname in os.listdir(os.path.join(folder, label)):\n",
    "                self.fnames.append(os.path.join(folder, label, fname))\n",
    "                labels.append(label)\n",
    "\n",
    "        assert len(labels) == len(self.fnames)\n",
    "        print('Number of {} videos: {:d}'.format(split, len(self.fnames)))\n",
    "\n",
    "        # Prepare a mapping between the label names (strings) and indices (ints)\n",
    "        self.label2index = {label: index for index, label in enumerate(sorted(set(labels)))}\n",
    "        # Convert the list of label names into an array of label indices\n",
    "        self.label_array = np.array([self.label2index[label] for label in labels], dtype=int)\n",
    "\n",
    "        if dataset == \"ucf101\":\n",
    "            if not os.path.exists('ucf_labels.txt'):\n",
    "                with open('ucf_labels.txt', 'w') as f:\n",
    "                    for id, label in enumerate(sorted(self.label2index)):\n",
    "                        f.writelines(str(id+1) + ' ' + label + '\\n')\n",
    "\n",
    "        elif dataset == 'hmdb51':\n",
    "            if not os.path.exists('hmdb_labels.txt'):\n",
    "                with open('hmdb_labels.txt', 'w') as f:\n",
    "                    for id, label in enumerate(sorted(self.label2index)):\n",
    "                        f.writelines(str(id+1) + ' ' + label + '\\n')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Loading and preprocessing.\n",
    "        buffer = self.load_frames(self.fnames[index])\n",
    "        buffer = self.crop(buffer, self.clip_len, self.crop_size)\n",
    "        labels = np.array(self.label_array[index])\n",
    "\n",
    "        if self.split == 'test':\n",
    "            # Perform data augmentation\n",
    "            buffer = self.randomflip(buffer)\n",
    "        buffer = self.normalize(buffer)\n",
    "        buffer = self.to_tensor(buffer)\n",
    "        return torch.from_numpy(buffer), torch.from_numpy(labels)\n",
    "\n",
    "    def check_integrity(self):\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def check_preprocess(self):\n",
    "        # TODO: Check image size in output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            return False\n",
    "        elif not os.path.exists(os.path.join(self.output_dir, 'train')):\n",
    "            return False\n",
    "\n",
    "        for ii, video_class in enumerate(os.listdir(os.path.join(self.output_dir, 'train'))):\n",
    "            for video in os.listdir(os.path.join(self.output_dir, 'train', video_class)):\n",
    "                video_name = os.path.join(os.path.join(self.output_dir, 'train', video_class, video),\n",
    "                                    sorted(os.listdir(os.path.join(self.output_dir, 'train', video_class, video)))[0])\n",
    "                image = cv2.imread(video_name)\n",
    "                if np.shape(image)[0] != 128 or np.shape(image)[1] != 171:\n",
    "                    return False\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if ii == 10:\n",
    "                break\n",
    "\n",
    "        return True\n",
    "\n",
    "    def preprocess(self):\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.mkdir(self.output_dir)\n",
    "            os.mkdir(os.path.join(self.output_dir, 'train'))\n",
    "            os.mkdir(os.path.join(self.output_dir, 'val'))\n",
    "            os.mkdir(os.path.join(self.output_dir, 'test'))\n",
    "\n",
    "        # Split train/val/test sets\n",
    "        for file in os.listdir(self.root_dir):\n",
    "            file_path = os.path.join(self.root_dir, file)\n",
    "            video_files = [name for name in os.listdir(file_path)]\n",
    "\n",
    "            train_and_valid, test = train_test_split(video_files, test_size=0.2, random_state=42)\n",
    "            train, val = train_test_split(train_and_valid, test_size=0.2, random_state=42)\n",
    "\n",
    "            train_dir = os.path.join(self.output_dir, 'train', file)\n",
    "            val_dir = os.path.join(self.output_dir, 'val', file)\n",
    "            test_dir = os.path.join(self.output_dir, 'test', file)\n",
    "\n",
    "            if not os.path.exists(train_dir):\n",
    "                os.mkdir(train_dir)\n",
    "            if not os.path.exists(val_dir):\n",
    "                os.mkdir(val_dir)\n",
    "            if not os.path.exists(test_dir):\n",
    "                os.mkdir(test_dir)\n",
    "\n",
    "            for video in train:\n",
    "                self.process_video(video, file, train_dir)\n",
    "\n",
    "            for video in val:\n",
    "                self.process_video(video, file, val_dir)\n",
    "\n",
    "            for video in test:\n",
    "                self.process_video(video, file, test_dir)\n",
    "\n",
    "        print('Preprocessing finished.')\n",
    "\n",
    "    def process_video(self, video, action_name, save_dir):\n",
    "        # Initialize a VideoCapture object to read video data into a numpy array\n",
    "        video_filename = video.split('.')[0]\n",
    "        if not os.path.exists(os.path.join(save_dir, video_filename)):\n",
    "            os.mkdir(os.path.join(save_dir, video_filename))\n",
    "\n",
    "        capture = cv2.VideoCapture(os.path.join(self.root_dir, action_name, video))\n",
    "\n",
    "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # Make sure splited video has at least 16 frames\n",
    "        EXTRACT_FREQUENCY = 4\n",
    "        if frame_count // EXTRACT_FREQUENCY <= 16:\n",
    "            EXTRACT_FREQUENCY -= 1\n",
    "            if frame_count // EXTRACT_FREQUENCY <= 16:\n",
    "                EXTRACT_FREQUENCY -= 1\n",
    "                if frame_count // EXTRACT_FREQUENCY <= 16:\n",
    "                    EXTRACT_FREQUENCY -= 1\n",
    "\n",
    "        count = 0\n",
    "        i = 0\n",
    "        retaining = True\n",
    "\n",
    "        while (count < frame_count and retaining):\n",
    "            retaining, frame = capture.read()\n",
    "            if frame is None:\n",
    "                continue\n",
    "\n",
    "            if count % EXTRACT_FREQUENCY == 0:\n",
    "                if (frame_height != self.resize_height) or (frame_width != self.resize_width):\n",
    "                    frame = cv2.resize(frame, (self.resize_width, self.resize_height))\n",
    "                cv2.imwrite(filename=os.path.join(save_dir, video_filename, '0000{}.jpg'.format(str(i))), img=frame)\n",
    "                i += 1\n",
    "            count += 1\n",
    "\n",
    "        # Release the VideoCapture once it is no longer needed\n",
    "        capture.release()\n",
    "\n",
    "    def randomflip(self, buffer):\n",
    "        \"\"\"Horizontally flip the given image and ground truth randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            for i, frame in enumerate(buffer):\n",
    "                frame = cv2.flip(buffer[i], flipCode=1)\n",
    "                buffer[i] = cv2.flip(frame, flipCode=1)\n",
    "\n",
    "        return buffer\n",
    "\n",
    "\n",
    "    def normalize(self, buffer):\n",
    "        for i, frame in enumerate(buffer):\n",
    "            frame -= np.array([[[90.0, 98.0, 102.0]]])\n",
    "            buffer[i] = frame\n",
    "\n",
    "        return buffer\n",
    "\n",
    "    def to_tensor(self, buffer):\n",
    "        return buffer.transpose((3, 0, 1, 2))\n",
    "\n",
    "    def load_frames(self, file_dir):\n",
    "        frames = sorted([os.path.join(file_dir, img) for img in os.listdir(file_dir)])\n",
    "        frame_count = len(frames)\n",
    "        buffer = np.empty((frame_count, self.resize_height, self.resize_width, 3), np.dtype('float32'))\n",
    "        for i, frame_name in enumerate(frames):\n",
    "            frame = np.array(cv2.imread(frame_name)).astype(np.float64)\n",
    "            buffer[i] = frame\n",
    "\n",
    "        return buffer\n",
    "\n",
    "    def crop(self, buffer, clip_len, crop_size):\n",
    "        # randomly select time index for temporal jittering\n",
    "        time_index = np.random.randint(buffer.shape[0] - clip_len)\n",
    "\n",
    "        # Randomly select start indices in order to crop the video\n",
    "        height_index = np.random.randint(buffer.shape[1] - crop_size)\n",
    "        width_index = np.random.randint(buffer.shape[2] - crop_size)\n",
    "\n",
    "        # Crop and jitter the video using indexing. The spatial crop is performed on\n",
    "        # the entire array, so each frame is cropped in the same location. The temporal\n",
    "        # jitter takes place via the selection of consecutive frames\n",
    "        buffer = buffer[time_index:time_index + clip_len,\n",
    "                 height_index:height_index + crop_size,\n",
    "                 width_index:width_index + crop_size, :]\n",
    "\n",
    "        return buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_data = VideoDataset(dataset='ucf101', split='test', clip_len=8, preprocess=False)\n",
    "print(len(train_data))\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "for i, sample in enumerate(train_loader):\n",
    "    inputs = sample[0]\n",
    "    labels = sample[1]\n",
    "    print(inputs.size())\n",
    "    print(labels)\n",
    "\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77845261",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(train_loader):\n",
    "    inputs = sample[0]\n",
    "    labels = sample[1]\n",
    "    print(inputs.size())\n",
    "    print(labels)\n",
    "\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11ee361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    filtered_batch = []\n",
    "    for video, _, label in batch:\n",
    "        filtered_batch.append((video, label))\n",
    "    return torch.utils.data.dataloader.default_collate(filtered_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501f3a7",
   "metadata": {},
   "source": [
    "## UCF101 Solved\n",
    "\n",
    "Problema en windows con las rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "335ee784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class UCF1012(UCF101):\n",
    "    def __init__(self, root, annotation_path, frames_per_clip, step_between_clips=1,\n",
    "                 frame_rate=None, fold=1, train=True, transform=None,\n",
    "                 _precomputed_metadata=None, num_workers=1, _video_width=0,\n",
    "                 _video_height=0, _video_min_dimension=0, _audio_samples=0):\n",
    "        \n",
    "        super(UCF1012, self).__init__(root, annotation_path, frames_per_clip, step_between_clips,\n",
    "                 frame_rate, fold, train, transform,\n",
    "                 _precomputed_metadata, num_workers, _video_width,\n",
    "                 _video_height, _video_min_dimension, _audio_samples)\n",
    "    \n",
    "    def _select_fold(self, video_list, annotation_path, fold, train):\n",
    "        name = \"train\" if train else \"test\"\n",
    "        name = \"{}list{:02d}.txt\".format(name, fold)\n",
    "        f = os.path.join(annotation_path, name)\n",
    "        selected_files = []\n",
    "        with open(f, \"r\") as fid:\n",
    "            data = fid.readlines()\n",
    "            data = [x.strip().split(\" \") for x in data]\n",
    "            data = [os.path.join(self.root, x[0]).replace('/', '\\\\') for x in data]\n",
    "            selected_files.extend(data)\n",
    "        \n",
    "        print(selected_files[0])\n",
    "        print(video_list[0])\n",
    "        \n",
    "        selected_files = set(selected_files)\n",
    "        \n",
    "        indices = [i for i in range(len(video_list)) if video_list[i] in selected_files]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f18bb",
   "metadata": {},
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "916f0bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\UFC101\\UCF101\\UCF-101\\ApplyEyeMakeup\\v_ApplyEyeMakeup_g08_c01.avi\n",
      "datasets\\UFC101\\UCF101\\UCF-101\\ApplyEyeMakeup\\v_ApplyEyeMakeup_g01_c01.avi\n"
     ]
    }
   ],
   "source": [
    "# create train loader (allowing batches and other extras)\n",
    "train_dataset = UCF1012(ucf_data_dir, \n",
    "                       ucf_label_dir, \n",
    "                       frames_per_clip=frames_per_clip,\n",
    "                       step_between_clips=step_between_clips,\n",
    "                       train=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb95d1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train samples: 1757468\n",
      "Total number of train samples: 54921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of train samples: {len(train_dataset)}\")\n",
    "print(f\"Total number of train samples: {len(train_loader)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ac78c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "torch.Size([5, 240, 320, 3]) x 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8048/1660242408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torchvision\\datasets\\ucf101.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_clips\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torchvision\\datasets\\video_utils.py\u001b[0m in \u001b[0;36mget_clip\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresampling_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"video_fps\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         assert len(video) == self.num_frames, \"{} x {}\".format(\n\u001b[0m\u001b[0;32m    380\u001b[0m             \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         )\n",
      "\u001b[1;31mAssertionError\u001b[0m: torch.Size([5, 240, 320, 3]) x 4"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbeb68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db06361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\UFC101\\UCF101\\UCF-101\\ApplyEyeMakeup\\v_ApplyEyeMakeup_g01_c01.avi\n",
      "datasets\\UFC101\\UCF101\\UCF-101\\ApplyEyeMakeup\\v_ApplyEyeMakeup_g01_c01.avi\n"
     ]
    }
   ],
   "source": [
    "test_dataset = UCF1012(ucf_data_dir,\n",
    "                       ucf_label_dir,\n",
    "                       frames_per_clip=frames_per_clip,\n",
    "                       step_between_clips=step_between_clips,\n",
    "                       train=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "693994c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8048/591905706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Total number of train samples: {len(test_dataset)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Total number of train samples: {len(test_loader)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of train samples: {len(test_dataset)}\")\n",
    "print(f\"Total number of train samples: {len(test_loader)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b3248a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "torch.Size([6, 240, 320, 3]) x 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4776/1660242408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torchvision\\datasets\\ucf101.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_clips\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NeuralCompu\\lib\\site-packages\\torchvision\\datasets\\video_utils.py\u001b[0m in \u001b[0;36mget_clip\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresampling_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"video_fps\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         assert len(video) == self.num_frames, \"{} x {}\".format(\n\u001b[0m\u001b[0;32m    380\u001b[0m             \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         )\n",
      "\u001b[1;31mAssertionError\u001b[0m: torch.Size([6, 240, 320, 3]) x 5"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8b9e4",
   "metadata": {},
   "source": [
    "# Conv3D test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e357733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded5d58e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4776/916507344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'functions'"
     ]
    }
   ],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, 4, 2, 1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv3d(512, 1, 4, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, 101),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x's size: batch_size * 1 * 64 * 64 * 64\n",
    "        return self.main(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94780a7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BasicBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4776/4190851751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'BasicBlock' is not defined"
     ]
    }
   ],
   "source": [
    "net = BasicBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2e5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa25880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604a466e",
   "metadata": {},
   "source": [
    "## Spiking Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba0a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
